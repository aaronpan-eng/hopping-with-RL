{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Function to run Stable-Baselines3 DDPG\n",
    "def run_SB3_DDPG(env_type, steps, save_dir):\n",
    "    env = gym.make(env_type)\n",
    "    env = Monitor(env)\n",
    "\n",
    "    # The noise objects for DDPG\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
    "    model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, batch_size=64, tau=0.001, verbose=1)\n",
    "    model.learn(total_timesteps=steps, log_interval=10)\n",
    "    model.save(f\"{save_dir}/SB3_DDPG_{env_type}\")\n",
    "\n",
    "    # training rewards\n",
    "    episode_rewards = env.get_episode_rewards()\n",
    "\n",
    "    return episode_rewards\n",
    "    \n",
    "\n",
    "environments = ['Hopper-v5', 'HalfCheetah-v5', 'BipedalWalker-v3']\n",
    "render_mode = ['human', 'human', 'human']\n",
    "steps = 1_000  # Total training steps\n",
    "save_dir = os.getcwd()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, env_type in enumerate(environments):\n",
    "    # # Run custom DDPG\n",
    "    # print(f\"Running custom DDPG on {env_type}...\")\n",
    "    # custom_scores = run_custom_DDPG(env_type, render_mode[i])\n",
    "\n",
    "    # Run Stable-Baselines3 DDPG\n",
    "    print(f\"Running Stable-Baselines3 DDPG on {env_type}...\")\n",
    "    sb3_scores = run_SB3_DDPG(env_type, steps, save_dir)\n",
    "\n",
    "    # Save results\n",
    "    results[env_type] = {\"sb3\": sb3_scores}\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure()\n",
    "    # plt.plot(np.arange(1, len(custom_scores) + 1), custom_scores, label=\"Custom DDPG\")\n",
    "    plt.plot(np.arange(1, len(sb3_scores) + 1), sb3_scores, label=\"SB3 DDPG\")\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.title(f\"Comparison of DDPG Implementations on {env_type}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_dir}/comparison_{env_type}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Save results to CSV\n",
    "for env, data in results.items():\n",
    "    pd.DataFrame({\n",
    "        \"SB3_DDPG\": data[\"sb3\"]\n",
    "    }).to_csv(f\"{save_dir}/results_comparison_{env}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RENDERING FROM STABLE BASELINES 3 ####\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "# Define environments and model paths\n",
    "environments = ['Hopper-v5', 'HalfCheetah-v5', 'BipedalWalker-v3']\n",
    "save_dir = os.getcwd()\n",
    "\n",
    "# Loop through environments and load corresponding models\n",
    "for env_type in environments:\n",
    "    # Load the environment\n",
    "    env = gym.make(env_type, render_mode=\"human\")\n",
    "    \n",
    "    # Load the model\n",
    "    model_path = f\"{save_dir}/SB3_DDPG_{env_type}.zip\"\n",
    "    model = DDPG.load(model_path, env=env)\n",
    "\n",
    "    # Start evaluation\n",
    "    obs = env.reset()[0]  # Extract initial observation from reset\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    print(f\"Evaluating {env_type}...\")\n",
    "    \n",
    "    while not done:\n",
    "        # Predict action and step through the environment\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncate, info = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "\n",
    "        # Render the environment\n",
    "        env.render()\n",
    "\n",
    "    print(f\"Cumulative reward for {env_type}: {cumulative_reward}\")\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
