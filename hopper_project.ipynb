{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the correct packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mujoco\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "# for copying deep nets to another variable\n",
    "from copy import deepcopy\n",
    "\n",
    "# library for ou noise as implemented with the paper\n",
    "from ou_noise import ou\n",
    "\n",
    "# to view model summary\n",
    "from torchsummary import summary\n",
    "\n",
    "# queue for replay buffer\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        # initialize parameters\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def insert(self, obs, action, reward, next_obs, done):\n",
    "        # tuple to represent transition\n",
    "        trans = (torch.tensor(obs, dtype=torch.float32), torch.tensor(action, dtype=torch.float32),\n",
    "                 torch.tensor(reward, dtype=torch.float32), torch.tensor(next_obs, dtype=torch.float32), torch.tensor(done, dtype=torch.float32))\n",
    "\n",
    "        # save transition to buffer\n",
    "        # use deque because once its full it discards old items\n",
    "        self.buffer.append(trans)\n",
    "\n",
    "    def sample_random_minibatch(self, batch_size, device):\n",
    "        # Random idx to sample from buffer w/o replacement\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Unpack batch into separate lists of tensors\n",
    "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
    "        \n",
    "        # Convert lists of tensors into single tensors\n",
    "        obs = torch.stack(obs).to(device)\n",
    "        actions = torch.stack(actions).to(device)\n",
    "        rewards = torch.stack(rewards).to(device)\n",
    "        next_obs = torch.stack(next_obs).to(device)\n",
    "        dones = torch.stack(dones).to(device)\n",
    "\n",
    "        # # convert list to tensor for easy slciing\n",
    "        # batch = torch.tensor(batch)\n",
    "\n",
    "        # # slicing to grab elements\n",
    "        # obs = batch[:,0]\n",
    "        # actions =  batch[:,1]\n",
    "        # rewards = batch[:,2]\n",
    "        # next_obs = batch[:,3]\n",
    "        # dones = batch[:,4]\n",
    "\n",
    "        # tuple of tensors\n",
    "        batch = (obs, actions, rewards, next_obs, dones)\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    # def prepopulate(self, env, actor):\n",
    "    #     # select action from actor\n",
    "    #     # execute action in the env\n",
    "    #     # store transition\n",
    "\n",
    "    #     # intialize state\n",
    "    #     state,_ = env.reset()\n",
    "\n",
    "    #     # loop through num_steps\n",
    "    #     for i in range(self.buffer_size):\n",
    "    #         # choose random action from environment action space (random policy)\n",
    "    #         action = env.action_space.sample()\n",
    "    #         # take action: get next state, reward, done\n",
    "    #         next_state, reward, done, truncate,_ = env.step(action)\n",
    "    #         # add transition to memory\n",
    "    #         self.insert(state, action, reward, next_state, done)\n",
    "\n",
    "    #         # update state\n",
    "    #         if done or truncate:\n",
    "    #             # if truncation reached, reset state\n",
    "    #             state,_ = env.reset()\n",
    "    #         else:\n",
    "    #             # all else state is the next\n",
    "    #             state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING REPLAY BUFFER\n",
    "# test = ReplayBuffer(5)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,False)\n",
    "\n",
    "# sample = test.sample_random_minibatch(3)\n",
    "# print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor AKA: The POLICY\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dims=(400,300), init_weight = 3e-3) -> None:\n",
    "        super(Actor, self).__init__()\n",
    "        # In the DDPG paper the parameters for the ACTOR are:\n",
    "        # - Learning rate: 10^-4\n",
    "        # - 2 hidden layers\n",
    "        # - 400 & 300 hidden dims (called units in paper) for first and second hidden layer, respectively\n",
    "        # - ReLU (rectified nonlinearity) for all hidden layers\n",
    "        # - output layer uses tanh (returns actions needed for the agent)\n",
    "\n",
    "        # initializing layer weights\n",
    "        # - hidden layers weights iniitalized with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # - output layer weights initialized with uniform distribution (-3e-3,3e-3)\n",
    "        self.init_weight_limit = init_weight\n",
    "\n",
    "        # hidden layers\n",
    "        self.hidden1 = nn.Linear(num_states, hidden_dims[0]) # input to hidden\n",
    "        self.hidden2 = nn.Linear(hidden_dims[0], hidden_dims[1]) # hidden to hidden\n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_dims[1], num_actions) # hidden to output\n",
    "        # activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input to first hidden layer w/ relu activation\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # feed into second hidden layer w/ relu activation\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # feed through output layer w/ tanh activation\n",
    "        x = self.output(x)\n",
    "        y = self.tanh(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # init hidden with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        self.hidden1.weight.data.uniform_(-(1/math.sqrt(self.hidden1.weight.size(1))),(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden2.weight.data.uniform_(-(1/math.sqrt(self.hidden2.weight.size(1))),(1/math.sqrt(self.hidden2.weight.size(1))))\n",
    "        # output layer weights init with uniform distribution (-3e-3,3e-3)\n",
    "        self.output.weight.data.uniform_(-self.init_weight_limit, self.init_weight_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic AKA: The Q-VALUE FUNCTION\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, output_dim=1, hidden_dims=(400,300), init_weight = 3e-3) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        # In the DDPG paper the parameters for the CRITIC are:\n",
    "        # - Learning rate: 10^-3\n",
    "        # - 2 hidden layers\n",
    "        # - 400 & 300 hidden dims (called units in paper) for first and second hidden layer, respectively\n",
    "        # - ReLU (rectified nonlinearity) for all hidden layers\n",
    "        # - output layer uses tanh (returns a single q-value for the input state-action pair)\n",
    "        # - output layer weights initialized with uniform distribution (low=-3e-3,high=3e-3)\n",
    "\n",
    "        # initializing layer weights\n",
    "        # - hidden layers weights iniitalized with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # - output layer weights initialized with uniform distribution (-3e-3,3e-3)\n",
    "        self.init_weight_limit = init_weight\n",
    "\n",
    "        # hidden layers\n",
    "        self.hidden1 = nn.Linear(num_states, hidden_dims[0]) # input to hidden, nn.Linear are the next layers after the given input x\n",
    "        self.hidden2 = nn.Linear(hidden_dims[0]+num_actions, hidden_dims[1]) # hidden to hidden\n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_dims[1], output_dim) # hidden to output\n",
    "        # activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pull state and action from input\n",
    "        state, action = x\n",
    "\n",
    "        # first hidden layer and relu activation\n",
    "        x = self.hidden1(state)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # in critic (Q-value fn) network, the actions are not included until the second hidden layer\n",
    "        # feed thru w/ relu activation\n",
    "        x = self.hidden2(torch.cat([x,action],1))\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # feed through output layer w/ tanh activation\n",
    "        x = self.output(x)\n",
    "        y = self.tanh(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # init hidden with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # alternative method: nn.init.uniform_(self.hidden1.weight, a=-(1/math.sqrt(self.hidden1.weight.size(1))), b=(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden1.weight.data.uniform_(-(1/math.sqrt(self.hidden1.weight.size(1))),(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden2.weight.data.uniform_(-(1/math.sqrt(self.hidden2.weight.size(1))),(1/math.sqrt(self.hidden2.weight.size(1))))\n",
    "        # output layer weights init with uniform distribution (-3e-3,3e-3)\n",
    "        self.output.weight.data.uniform_(-self.init_weight_limit, self.init_weight_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [reference] https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-bipedal/ddpg_agent.py\n",
    "# Used Udacity tutorial for OU noise generation\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return torch.tensor(self.state, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, env, params, random_seed) -> None:\n",
    "        # grabbing parameters\n",
    "        self.gamma = params['gamma']\n",
    "        self.tau = params['tau']\n",
    "        self.actor_lr = params['actor_lr']\n",
    "        self.critic_lr = params['critic_lr']\n",
    "        self.batch_size = params['minibatch_size']\n",
    "        self.buffer_size = params['replay_buffer_size']\n",
    "        self.weight_decay = params['L2_weight_decay']\n",
    "\n",
    "        # setting random seeds\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed) # setting for cuda if GPU available\n",
    "\n",
    "        # setting number of states and actions\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.shape[0]\n",
    "\n",
    "        # choose device\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        # initialize critic network w target\n",
    "        self.critic = Critic(self.num_states, self.num_actions).to(self.device)\n",
    "        # summary(self.critic, input_size=(2))\n",
    "        # creating deepcopy to copy the network over to a target\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        # define optimizer\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.critic_lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        # initialize actor network w target\n",
    "        self.actor = Actor(self.num_states, self.num_actions).to(self.device)\n",
    "        # summary(self.actor, input_size=(11,))\n",
    "        # creating deepcopy to copy the network over to a target\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        # define optimizer\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "\n",
    "        # OU noise for action selection\n",
    "        self.noise = OUNoise(self.num_actions, random_seed)\n",
    "\n",
    "        # initialize replay buffer and prepopulate\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "        # self.replay_buffer.prepopulate()\n",
    "\n",
    "    # get action with some noise\n",
    "    def get_action(self, env, state):\n",
    "        # convert to tensor to feed into network\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # set to eval mode to not track batch norm\n",
    "        self.actor.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "            action += self.noise.sample()\n",
    "        self.actor.train()\n",
    "        \n",
    "        # copy to cpu if necessary\n",
    "        # convert to numpy for OpenAI step input\n",
    "        action = action.cpu().numpy()\n",
    "        action = np.clip(action,env.action_space.low,env.action_space.high)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    # updates critic and actor\n",
    "    def update(self):\n",
    "        # sample batch\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, dones_batch = self.replay_buffer.sample_random_minibatch(self.batch_size, self.device)\n",
    "\n",
    "        # calculate target batch\n",
    "        # with torch.no_grad():\n",
    "        target = self.calculate_target(reward_batch, next_state_batch, dones_batch)\n",
    "\n",
    "        # calculate q-value batch\n",
    "        q_val_batch = self.critic((state_batch, action_batch))\n",
    "\n",
    "        # updating critic: by minimizing loss\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        loss_val = loss(q_val_batch, target)\n",
    "        loss_val.backward()        \n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # updating actor: using critic to update actor\n",
    "        loss_actor = -self.critic((state_batch, self.actor(state_batch))) # TODO: should this be negative?\n",
    "        \n",
    "        self.actor.zero_grad()\n",
    "        loss_actor = torch.mean(loss_actor)\n",
    "        loss_actor.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # update target network weights\n",
    "        # update target critic\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )\n",
    "        \n",
    "        # update target actor\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )\n",
    "\n",
    "        return loss_actor.item()\n",
    "\n",
    "    def calculate_target(self, reward, next_state, dones):\n",
    "        next_action = self.actor_target(next_state)\n",
    "        target = reward + self.gamma*self.critic_target((next_state, next_action))*(1-dones)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# main loop\n",
    "def run_DDPG():\n",
    "    # initialize parameters\n",
    "    params = {'actor_lr': 0.0001,\n",
    "            'critic_lr': 0.001,\n",
    "            'tau': 0.0001,\n",
    "            'gamma': 0.99,\n",
    "            'minibatch_size': 64,\n",
    "            'replay_buffer_size': int(1e6),\n",
    "            'steps': 2_000,\n",
    "            'L2_weight_decay': 1e-2}\n",
    "\n",
    "    # grab cwd for model saving\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # create environment\n",
    "    env = gym.make('Hopper-v5', render_mode='human')\n",
    "\n",
    "    # ddpg object\n",
    "    ddpg = DDPGAgent(env, params, random_seed=10)\n",
    "\n",
    "    # keep track of loss\n",
    "    scores = []\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    loss = []\n",
    "\n",
    "    # loop through desired number of steps\n",
    "    with tqdm(range(params['steps']), desc=\"Episode\") as pbar:\n",
    "        for ep in pbar:\n",
    "            # reset env\n",
    "            state,_ = env.reset()\n",
    "\n",
    "            # initialize terminal state\n",
    "            done = False\n",
    "\n",
    "            # track cumulative reward\n",
    "            cumulative_reward = 0\n",
    "\n",
    "            # while environment isnt terminal\n",
    "            while not done:\n",
    "                # grab action with OU noise\n",
    "                action = ddpg.get_action(env, state)\n",
    "                # execute action in env\n",
    "                next_state, reward, done, truncate, _ = env.step(action)\n",
    "                # store in buffer\n",
    "                ddpg.replay_buffer.insert(state, action, reward, next_state, done)\n",
    "\n",
    "                # learn when buffer reaches batch size\n",
    "                if len(ddpg.replay_buffer.buffer) > ddpg.batch_size:\n",
    "                    loss_item = ddpg.update()\n",
    "                    loss.append(loss_item)\n",
    "\n",
    "                # update state\n",
    "                state = next_state\n",
    "\n",
    "                # update cumulative reward\n",
    "                cumulative_reward += reward\n",
    "\n",
    "            # append to running score and to score deque for average reward approximation\n",
    "            scores.append(cumulative_reward)\n",
    "            scores_deque.append(cumulative_reward)\n",
    "            pbar.set_description(f\"Episode {ep + 1}\")\n",
    "            pbar.set_postfix(Avg_Score=np.mean(scores_deque), Episode_Score=cumulative_reward)\n",
    "            # print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpisode score: {:.2f}'.format(ep, np.mean(scores_deque), cumulative_reward), end=\"\")\n",
    "            # save scores\n",
    "            if ep % 100 == 0:\n",
    "                torch.save(ddpg.actor.state_dict(), cwd+'/checkpoint_actor.pth')\n",
    "                torch.save(ddpg.critic.state_dict(), cwd+'/checkpoint_critic.pth')\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(ep, np.mean(scores_deque)))   \n",
    "            \n",
    "            # reset env if done\n",
    "            env.reset()\n",
    "\n",
    "    \n",
    "    return scores, loss\n",
    "\n",
    "\n",
    "scores, loss = run_DDPG()\n",
    "\n",
    "plt.plot(np.arange(1,len(scores)+1), scores)\n",
    "plt.ylabel('Cumulative reward: ')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1:   0%|          | 1/2000 [00:00<26:56,  1.24it/s, Avg_Score=44.8, Episode_Score=44.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: 44.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 9:   0%|          | 9/2000 [00:04<15:35,  2.13it/s, Avg_Score=40.3, Episode_Score=38.1]c:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\glfw\\__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n",
      "Episode 101:   5%|▌         | 101/2000 [00:28<11:30,  2.75it/s, Avg_Score=37.9, Episode_Score=37.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 37.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 202:  10%|█         | 202/2000 [00:51<05:40,  5.29it/s, Avg_Score=38, Episode_Score=39.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200\tAverage Score: 37.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 301:  15%|█▌        | 301/2000 [01:10<05:50,  4.84it/s, Avg_Score=37.9, Episode_Score=39.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300\tAverage Score: 37.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 402:  20%|██        | 401/2000 [01:31<05:30,  4.83it/s, Avg_Score=38, Episode_Score=37.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400\tAverage Score: 37.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 501:  25%|██▌       | 501/2000 [01:55<06:55,  3.61it/s, Avg_Score=38, Episode_Score=37.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500\tAverage Score: 37.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 601:  30%|███       | 601/2000 [02:19<04:32,  5.13it/s, Avg_Score=37.9, Episode_Score=37.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600\tAverage Score: 37.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 702:  35%|███▌      | 701/2000 [02:38<04:16,  5.07it/s, Avg_Score=37.7, Episode_Score=39.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700\tAverage Score: 37.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 801:  40%|████      | 801/2000 [02:59<05:00,  3.99it/s, Avg_Score=37.7, Episode_Score=37.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800\tAverage Score: 37.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 901:  45%|████▌     | 901/2000 [03:22<03:58,  4.61it/s, Avg_Score=37.9, Episode_Score=38.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900\tAverage Score: 37.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 903:  45%|████▌     | 903/2000 [03:22<04:06,  4.45it/s, Avg_Score=37.9, Episode_Score=37.3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m render_mode \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, env_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(environments):\n\u001b[1;32m---> 89\u001b[0m     scores, loss, steps \u001b[38;5;241m=\u001b[39m \u001b[43mrun_DDPG\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,steps)\n\u001b[0;32m     92\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(scores)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), scores)\n",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m, in \u001b[0;36mrun_DDPG\u001b[1;34m(env_type, mode)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# learn when buffer reaches batch size\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ddpg\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m ddpg\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m---> 52\u001b[0m     loss_item \u001b[38;5;241m=\u001b[39m \u001b[43mddpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     loss\u001b[38;5;241m.\u001b[39mappend(loss_item)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# update state\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 92\u001b[0m, in \u001b[0;36mDDPGAgent.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     91\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m loss(q_val_batch, target)\n\u001b[1;32m---> 92\u001b[0m \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# updating actor: using critic to update actor\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# main loop\n",
    "def run_DDPG(env_type, mode):\n",
    "    # initialize parameters\n",
    "    params = {'actor_lr': 0.0001,\n",
    "            'critic_lr': 0.001,\n",
    "            'tau': 0.0001,\n",
    "            'gamma': 0.99,\n",
    "            'minibatch_size': 64,\n",
    "            'replay_buffer_size': int(1e6),\n",
    "            'steps': 2_000,\n",
    "            'L2_weight_decay': 1e-2}\n",
    "\n",
    "    # grab cwd for model saving\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # create environment\n",
    "    env = gym.make(env_type, render_mode=mode)\n",
    "\n",
    "    # ddpg object\n",
    "    ddpg = DDPGAgent(env, params, random_seed=10)\n",
    "\n",
    "    # keep track of loss\n",
    "    scores = []\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    loss = []\n",
    "    steps = 0\n",
    "\n",
    "    # loop through desired number of steps\n",
    "    with tqdm(range(params['steps']), desc=\"Episode\") as pbar:\n",
    "        for ep in pbar:\n",
    "            # reset env\n",
    "            state,_ = env.reset()\n",
    "\n",
    "            # initialize terminal state\n",
    "            done = False\n",
    "\n",
    "            # track cumulative reward\n",
    "            cumulative_reward = 0\n",
    "\n",
    "            # while environment isnt terminal\n",
    "            while not done:\n",
    "                # grab action with OU noise\n",
    "                action = ddpg.get_action(env, state)\n",
    "                # execute action in env\n",
    "                next_state, reward, done, truncate, _ = env.step(action)\n",
    "                steps += 1\n",
    "                # store in buffer\n",
    "                ddpg.replay_buffer.insert(state, action, reward, next_state, done)\n",
    "\n",
    "                # learn when buffer reaches batch size\n",
    "                if len(ddpg.replay_buffer.buffer) > ddpg.batch_size:\n",
    "                    loss_item = ddpg.update()\n",
    "                    loss.append(loss_item)\n",
    "\n",
    "                # update state\n",
    "                state = next_state\n",
    "\n",
    "                # update cumulative reward\n",
    "                cumulative_reward += reward\n",
    "\n",
    "            # append to running score and to score deque for average reward approximation\n",
    "            scores.append(cumulative_reward)\n",
    "            scores_deque.append(cumulative_reward)\n",
    "            pbar.set_description(f\"Episode {ep + 1}\")\n",
    "            pbar.set_postfix(Avg_Score=np.mean(scores_deque), Episode_Score=cumulative_reward)\n",
    "            # print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpisode score: {:.2f}'.format(ep, np.mean(scores_deque), cumulative_reward), end=\"\")\n",
    "            # save scores\n",
    "            if ep % 100 == 0:\n",
    "                torch.save(ddpg.actor.state_dict(), f'{cwd}/checkpoint_{env_type}_actor.pth')\n",
    "                torch.save(ddpg.critic.state_dict(), f'{cwd}/checkpoint_{env_type}_critic.pth')\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(ep, np.mean(scores_deque)))   \n",
    "            \n",
    "            # reset env if done\n",
    "            env.reset()\n",
    "\n",
    "    # save scores and loss\n",
    "    scores_df = pd.DataFrame({\"Episode\": np.arange(1, len(scores) + 1), \"Scores\": scores})\n",
    "    loss_df = pd.DataFrame({\"Step\": np.arange(1, len(loss) + 1), \"Loss\": loss})\n",
    "\n",
    "    scores_df.to_csv(f\"{cwd}/results_{env_type}_scores.csv\", index=False)\n",
    "    loss_df.to_csv(f\"{cwd}/results_{env_type}_loss.csv\", index=False)\n",
    "    \n",
    "    return scores, loss, steps\n",
    "\n",
    "environments = ['Hopper-v5', 'HalfCheetah-v5', 'BipedalWalker-v3','CartPole-v1']\n",
    "render_mode = ['human','human','human','human']\n",
    "\n",
    "for i, env_type in enumerate(environments):\n",
    "    scores, loss, steps = run_DDPG(env_type, render_mode[i])\n",
    "    print(\"steps\",steps)\n",
    "\n",
    "    plt.plot(np.arange(1,len(scores)+1), scores)\n",
    "    plt.title(f\"Cumulative reward for {env_type}\")\n",
    "    plt.ylabel('Cumulative reward')\n",
    "    plt.xlabel('Episodes')\n",
    "    plot_file = f\"{os.getcwd()}/plot_{env_type}_scores.png\"\n",
    "    plt.savefig(plot_file)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# grab cwd for model saving\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# instantiate env\n",
    "env = gym.make('Hopper-v5', render_mode='human')\n",
    "\n",
    "# initialize parameters\n",
    "params = {'actor_lr': 0.0001,\n",
    "        'critic_lr': 0.001,\n",
    "        'tau': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'minibatch_size': 64,\n",
    "        'replay_buffer_size': int(10e6),\n",
    "        'steps': 100_000}\n",
    "\n",
    "ddpg = DDPGAgent(env, params, random_seed=10)\n",
    "\n",
    "ddpg.actor.load_state_dict(torch.load(cwd+'/checkpoint_actor.pth'))\n",
    "ddpg.critic.load_state_dict(torch.load(cwd+'/checkpoint_critic.pth'))\n",
    "\n",
    "state,_ = env.reset()  \n",
    "while True:\n",
    "    action = ddpg.get_action(env, state)\n",
    "    env.render()\n",
    "    next_state, reward, done, truncate, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stable-Baselines3 DDPG on Hopper-v5...\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 13.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 2546     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 80       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.2     |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 170      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.64    |\n",
      "|    critic_loss     | 0.0777   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 69       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | 27.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 258      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.62    |\n",
      "|    critic_loss     | 0.0932   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 157      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 105      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 331      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.58    |\n",
      "|    critic_loss     | 0.137    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 230      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.4     |\n",
      "|    ep_rew_mean     | 16.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 407      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.56    |\n",
      "|    critic_loss     | 0.0886   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 306      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.2     |\n",
      "|    ep_rew_mean     | 13.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 484      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.73    |\n",
      "|    critic_loss     | 0.0524   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 383      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | 13.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 634      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.88    |\n",
      "|    critic_loss     | 0.191    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 533      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.7     |\n",
      "|    ep_rew_mean     | 18.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 789      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.38    |\n",
      "|    critic_loss     | 0.322    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 688      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.4     |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.1     |\n",
      "|    critic_loss     | 0.381    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.1     |\n",
      "|    ep_rew_mean     | 23.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 1003     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.02    |\n",
      "|    critic_loss     | 0.457    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 902      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.9     |\n",
      "|    ep_rew_mean     | 25       |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 1094     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.68    |\n",
      "|    critic_loss     | 0.245    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 993      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.6     |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 1183     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.26    |\n",
      "|    critic_loss     | 0.191    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1082     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.5     |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 1273     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.98    |\n",
      "|    critic_loss     | 0.19     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1172     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.4     |\n",
      "|    ep_rew_mean     | 27.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 1364     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.34    |\n",
      "|    critic_loss     | 0.119    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1263     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.2     |\n",
      "|    ep_rew_mean     | 28.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 1452     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.5    |\n",
      "|    critic_loss     | 0.11     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1351     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.1     |\n",
      "|    ep_rew_mean     | 29.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 1541     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.6    |\n",
      "|    critic_loss     | 0.0796   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1440     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.1     |\n",
      "|    ep_rew_mean     | 29.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 1641     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.3    |\n",
      "|    critic_loss     | 0.217    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1540     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.5     |\n",
      "|    ep_rew_mean     | 30.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 1765     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.4    |\n",
      "|    critic_loss     | 0.33     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1664     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.9     |\n",
      "|    ep_rew_mean     | 30.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 1891     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.6    |\n",
      "|    critic_loss     | 0.25     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1790     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.2     |\n",
      "|    ep_rew_mean     | 31.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 2017     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.1    |\n",
      "|    critic_loss     | 0.168    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1916     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.5     |\n",
      "|    ep_rew_mean     | 31.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 2141     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.1    |\n",
      "|    critic_loss     | 0.148    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2040     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.8     |\n",
      "|    ep_rew_mean     | 32.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 2268     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.1    |\n",
      "|    critic_loss     | 0.237    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2167     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26       |\n",
      "|    ep_rew_mean     | 32.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 2393     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.9    |\n",
      "|    critic_loss     | 0.126    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2292     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.2     |\n",
      "|    ep_rew_mean     | 33       |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 2518     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.2    |\n",
      "|    critic_loss     | 0.123    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2417     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.4     |\n",
      "|    ep_rew_mean     | 33.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 2643     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.1    |\n",
      "|    critic_loss     | 0.184    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2542     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.9     |\n",
      "|    ep_rew_mean     | 34.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 2770     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.8    |\n",
      "|    critic_loss     | 0.306    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2669     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.3     |\n",
      "|    ep_rew_mean     | 34.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 2898     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.5    |\n",
      "|    critic_loss     | 0.17     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2797     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.5     |\n",
      "|    ep_rew_mean     | 35       |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 74       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 3010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.2    |\n",
      "|    critic_loss     | 0.516    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2909     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.6     |\n",
      "|    ep_rew_mean     | 38.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 74       |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 3193     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.8    |\n",
      "|    critic_loss     | 0.446    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3092     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.6     |\n",
      "|    ep_rew_mean     | 41.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 74       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 3363     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.464    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3262     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.3     |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 3614     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.8    |\n",
      "|    critic_loss     | 3.46     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3513     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32.1     |\n",
      "|    ep_rew_mean     | 49.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 3847     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28      |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3746     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 34.2     |\n",
      "|    ep_rew_mean     | 54       |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 4210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -30      |\n",
      "|    critic_loss     | 0.75     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4109     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37.1     |\n",
      "|    ep_rew_mean     | 60.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 4626     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -33.4    |\n",
      "|    critic_loss     | 5.78     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4525     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 40.4     |\n",
      "|    ep_rew_mean     | 66.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 5043     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -35.7    |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4942     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 43.3     |\n",
      "|    ep_rew_mean     | 72.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 5427     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -38.1    |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5326     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 46.5     |\n",
      "|    ep_rew_mean     | 79.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 5828     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -42.7    |\n",
      "|    critic_loss     | 0.769    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5727     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.9     |\n",
      "|    ep_rew_mean     | 86       |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 6259     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -44.8    |\n",
      "|    critic_loss     | 5.3      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6158     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53.3     |\n",
      "|    ep_rew_mean     | 93.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 6696     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -45.8    |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6595     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 56.3     |\n",
      "|    ep_rew_mean     | 99.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 7081     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -52.4    |\n",
      "|    critic_loss     | 3.02     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 58.5     |\n",
      "|    ep_rew_mean     | 103      |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 7390     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53      |\n",
      "|    critic_loss     | 12.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7289     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 61.9     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 7826     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -55.9    |\n",
      "|    critic_loss     | 2.56     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7725     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.2     |\n",
      "|    ep_rew_mean     | 116      |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 8190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.9    |\n",
      "|    critic_loss     | 6.43     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8089     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.8     |\n",
      "|    ep_rew_mean     | 120      |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 8475     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -62.3    |\n",
      "|    critic_loss     | 2.18     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8374     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 68.1     |\n",
      "|    ep_rew_mean     | 125      |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 120      |\n",
      "|    total_timesteps | 8831     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -62.6    |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8730     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.9     |\n",
      "|    ep_rew_mean     | 131      |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 126      |\n",
      "|    total_timesteps | 9232     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63      |\n",
      "|    critic_loss     | 2.92     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9131     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.8     |\n",
      "|    ep_rew_mean     | 138      |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 9650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -68.5    |\n",
      "|    critic_loss     | 7.1      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9549     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.9     |\n",
      "|    ep_rew_mean     | 145      |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 10085    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -72.5    |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9984     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 80.1     |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 10524    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -72      |\n",
      "|    critic_loss     | 4.26     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10423    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.1     |\n",
      "|    ep_rew_mean     | 159      |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 10956    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -74.6    |\n",
      "|    critic_loss     | 9.38     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10855    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.2     |\n",
      "|    ep_rew_mean     | 166      |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 160      |\n",
      "|    total_timesteps | 11394    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.4    |\n",
      "|    critic_loss     | 3.75     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11293    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.1     |\n",
      "|    ep_rew_mean     | 173      |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 11805    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.7    |\n",
      "|    critic_loss     | 3.22     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11704    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.4     |\n",
      "|    ep_rew_mean     | 173      |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 11948    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -80.4    |\n",
      "|    critic_loss     | 9.15     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11847    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.8     |\n",
      "|    ep_rew_mean     | 170      |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 11972    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.4    |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11871    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.3     |\n",
      "|    ep_rew_mean     | 168      |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 11996    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -84.9    |\n",
      "|    critic_loss     | 5.77     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11895    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.1     |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 171      |\n",
      "|    total_timesteps | 12020    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -80.9    |\n",
      "|    critic_loss     | 10.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11919    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82       |\n",
      "|    ep_rew_mean     | 159      |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 171      |\n",
      "|    total_timesteps | 12044    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -82.6    |\n",
      "|    critic_loss     | 19.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11943    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78.6     |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 12068    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -82.5    |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11967    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.7     |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 12092    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -79.8    |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11991    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.7     |\n",
      "|    ep_rew_mean     | 136      |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 12116    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -82      |\n",
      "|    critic_loss     | 3.65     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12015    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 67.1     |\n",
      "|    ep_rew_mean     | 129      |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 12140    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -80.6    |\n",
      "|    critic_loss     | 5.76     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12039    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 63.4     |\n",
      "|    ep_rew_mean     | 121      |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 12164    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -80      |\n",
      "|    critic_loss     | 4.99     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12063    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 59.3     |\n",
      "|    ep_rew_mean     | 113      |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 12188    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -84.8    |\n",
      "|    critic_loss     | 5.27     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12087    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 55.2     |\n",
      "|    ep_rew_mean     | 104      |\n",
      "| time/              |          |\n",
      "|    episodes        | 256      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 12212    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -80.7    |\n",
      "|    critic_loss     | 4.33     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12111    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 51.5     |\n",
      "|    ep_rew_mean     | 96.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 12236    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -83.3    |\n",
      "|    critic_loss     | 5.67     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12135    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.7     |\n",
      "|    ep_rew_mean     | 90.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 264      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 12260    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -82.3    |\n",
      "|    critic_loss     | 5.35     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12159    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 44.6     |\n",
      "|    ep_rew_mean     | 82.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 268      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 12284    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -84.1    |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12183    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 41.2     |\n",
      "|    ep_rew_mean     | 75.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 272      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 12310    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -83.6    |\n",
      "|    critic_loss     | 18       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12209    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 39.7     |\n",
      "|    ep_rew_mean     | 72.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 276      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 12443    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -84      |\n",
      "|    critic_loss     | 62.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12342    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.1     |\n",
      "|    ep_rew_mean     | 80.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 203      |\n",
      "|    total_timesteps | 13841    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -97.2    |\n",
      "|    critic_loss     | 7.63     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13740    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53.2     |\n",
      "|    ep_rew_mean     | 78.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 284      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 14550    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -111     |\n",
      "|    critic_loss     | 5.21     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14449    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53.2     |\n",
      "|    ep_rew_mean     | 73.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 288      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 220      |\n",
      "|    total_timesteps | 14975    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -112     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14874    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 51.8     |\n",
      "|    ep_rew_mean     | 66.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 292      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 225      |\n",
      "|    total_timesteps | 15269    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -117     |\n",
      "|    critic_loss     | 14.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15168    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 51.1     |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 296      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 15633    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -123     |\n",
      "|    critic_loss     | 70.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15532    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 51       |\n",
      "|    ep_rew_mean     | 57.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 16054    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -128     |\n",
      "|    critic_loss     | 9.59     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15953    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.4     |\n",
      "|    ep_rew_mean     | 54.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 304      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 242      |\n",
      "|    total_timesteps | 16434    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -126     |\n",
      "|    critic_loss     | 81       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16333    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.7     |\n",
      "|    ep_rew_mean     | 53.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 308      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 248      |\n",
      "|    total_timesteps | 16873    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -130     |\n",
      "|    critic_loss     | 51.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16772    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53.5     |\n",
      "|    ep_rew_mean     | 58.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 312      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 17293    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -132     |\n",
      "|    critic_loss     | 86.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17192    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 59.1     |\n",
      "|    ep_rew_mean     | 65       |\n",
      "| time/              |          |\n",
      "|    episodes        | 316      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 17884    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -140     |\n",
      "|    critic_loss     | 34.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17783    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.9     |\n",
      "|    ep_rew_mean     | 73.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 18482    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -145     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18381    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.4     |\n",
      "|    ep_rew_mean     | 81.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 324      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 279      |\n",
      "|    total_timesteps | 19061    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -148     |\n",
      "|    critic_loss     | 104      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 75.2     |\n",
      "|    ep_rew_mean     | 89.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 328      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 286      |\n",
      "|    total_timesteps | 19564    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -156     |\n",
      "|    critic_loss     | 84.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19463    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78.1     |\n",
      "|    ep_rew_mean     | 93.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 332      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 290      |\n",
      "|    total_timesteps | 19877    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -159     |\n",
      "|    critic_loss     | 19.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19776    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 80.6     |\n",
      "|    ep_rew_mean     | 96.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 336      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 20156    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -161     |\n",
      "|    critic_loss     | 49       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20055    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.2     |\n",
      "|    ep_rew_mean     | 101      |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 298      |\n",
      "|    total_timesteps | 20434    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -154     |\n",
      "|    critic_loss     | 75.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20333    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87       |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    episodes        | 344      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 303      |\n",
      "|    total_timesteps | 20840    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -160     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20739    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.7     |\n",
      "|    ep_rew_mean     | 111      |\n",
      "| time/              |          |\n",
      "|    episodes        | 348      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 307      |\n",
      "|    total_timesteps | 21137    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -161     |\n",
      "|    critic_loss     | 30       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21036    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.5     |\n",
      "|    ep_rew_mean     | 114      |\n",
      "| time/              |          |\n",
      "|    episodes        | 352      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 310      |\n",
      "|    total_timesteps | 21339    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 18.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21238    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.2     |\n",
      "|    ep_rew_mean     | 120      |\n",
      "| time/              |          |\n",
      "|    episodes        | 356      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 314      |\n",
      "|    total_timesteps | 21628    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -167     |\n",
      "|    critic_loss     | 55.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21527    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.5     |\n",
      "|    ep_rew_mean     | 129      |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 322      |\n",
      "|    total_timesteps | 22191    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -167     |\n",
      "|    critic_loss     | 19.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 22090    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | 141      |\n",
      "| time/              |          |\n",
      "|    episodes        | 364      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 332      |\n",
      "|    total_timesteps | 22896    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -179     |\n",
      "|    critic_loss     | 26       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 22795    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | 145      |\n",
      "| time/              |          |\n",
      "|    episodes        | 368      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 339      |\n",
      "|    total_timesteps | 23423    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -175     |\n",
      "|    critic_loss     | 91.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23322    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 62\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, env_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(environments):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# # Run custom DDPG\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# print(f\"Running custom DDPG on {env_type}...\")\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# custom_scores = run_custom_DDPG(env_type, render_mode[i])\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Run Stable-Baselines3 DDPG\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Stable-Baselines3 DDPG on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m     sb3_scores \u001b[38;5;241m=\u001b[39m \u001b[43mrun_SB3_DDPG\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     results[env_type] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msb3\u001b[39m\u001b[38;5;124m\"\u001b[39m: sb3_scores}\n",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m, in \u001b[0;36mrun_SB3_DDPG\u001b[1;34m(env_type, steps, save_dir)\u001b[0m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m SB3DDPG(\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/SB3_DDPG_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ddpg\\ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[0;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\td3\\td3.py:198\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    195\u001b[0m actor_losses\u001b[38;5;241m.\u001b[39mappend(actor_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Optimize the actor\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:815\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    813\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_zero_grad_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_groups\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\profiler.py:605\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DDPG as SB3DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to run Stable-Baselines3 DDPG\n",
    "def run_SB3_DDPG(env_type, steps, save_dir):\n",
    "    env = gym.make(env_type)\n",
    "    env = Monitor(env)  # To record statistics\n",
    "\n",
    "    # Set up action noise\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "    # Initialize Stable-Baselines3 DDPG\n",
    "    model = SB3DDPG(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        action_noise=action_noise,\n",
    "        learning_rate=1e-3,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=steps)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(f\"{save_dir}/SB3_DDPG_{env_type}\")\n",
    "\n",
    "    # Retrieve training rewards\n",
    "    episode_rewards = env.get_episode_rewards()\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "# Function to run your custom DDPG\n",
    "def run_custom_DDPG(env_type, mode):\n",
    "    scores, _ = run_DDPG(env_type, mode)\n",
    "    return scores\n",
    "\n",
    "# Comparison script\n",
    "environments = ['Hopper-v5', 'HalfCheetah-v5', 'BipedalWalker-v3', 'CartPole-v1']\n",
    "render_mode = ['human', 'human', 'human', 'human']\n",
    "steps = 100_000  # Total training steps\n",
    "save_dir = os.getcwd()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, env_type in enumerate(environments):\n",
    "    # # Run custom DDPG\n",
    "    # print(f\"Running custom DDPG on {env_type}...\")\n",
    "    # custom_scores = run_custom_DDPG(env_type, render_mode[i])\n",
    "\n",
    "    # Run Stable-Baselines3 DDPG\n",
    "    print(f\"Running Stable-Baselines3 DDPG on {env_type}...\")\n",
    "    sb3_scores = run_SB3_DDPG(env_type, steps, save_dir)\n",
    "\n",
    "    # Save results\n",
    "    results[env_type] = {\"sb3\": sb3_scores}\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure()\n",
    "    # plt.plot(np.arange(1, len(custom_scores) + 1), custom_scores, label=\"Custom DDPG\")\n",
    "    plt.plot(np.arange(1, len(sb3_scores) + 1), sb3_scores, label=\"SB3 DDPG\")\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.title(f\"Comparison of DDPG Implementations on {env_type}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_dir}/comparison_{env_type}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Save results to CSV\n",
    "for env, data in results.items():\n",
    "    pd.DataFrame({\n",
    "        \"SB3_DDPG\": data[\"sb3\"]\n",
    "    }).to_csv(f\"{save_dir}/results_comparison_{env}.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
