{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the correct packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mujoco\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import tqdm\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "# for copying deep nets to another variable\n",
    "from copy import deepcopy\n",
    "\n",
    "# library for ou noise as implemented with the paper\n",
    "from ou_noise import ou\n",
    "\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        # initialize parameters\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def insert(self, obs, action, reward, next_obs, done):\n",
    "        # tuple to represent transition\n",
    "        trans = (torch.tensor(obs), torch.tensor(action),\n",
    "                 torch.tensor(reward), torch.tensor(next_obs), torch.tensor(done))\n",
    "\n",
    "        # save transition to buffer\n",
    "        # use deque because once its full it discards old items\n",
    "        self.buffer.append(trans)\n",
    "\n",
    "    def sample_random_minibatch(self, batch_size):\n",
    "        # Random idx to sample from buffer w/o replacement\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        # convert list to tensor for easy slciing\n",
    "        batch = torch.tensor(batch)\n",
    "        \n",
    "\n",
    "        # slicing to grab elements\n",
    "        obs = batch[:,0]\n",
    "        actions =  batch[:,1]\n",
    "        rewards = batch[:,2]\n",
    "        next_obs = batch[:,3]\n",
    "        dones = batch[:,4]\n",
    "\n",
    "        # tuple of tensors\n",
    "        batch = (obs, actions, rewards, next_obs, dones)\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    # def prepopulate(self, env, actor):\n",
    "    #     # select action from actor\n",
    "    #     # execute action in the env\n",
    "    #     # store transition\n",
    "\n",
    "    #     # intialize state\n",
    "    #     state,_ = env.reset()\n",
    "\n",
    "    #     # loop through num_steps\n",
    "    #     for i in range(self.buffer_size):\n",
    "    #         # choose random action from environment action space (random policy)\n",
    "    #         action = env.action_space.sample()\n",
    "    #         # take action: get next state, reward, done\n",
    "    #         next_state, reward, done, truncate,_ = env.step(action)\n",
    "    #         # add transition to memory\n",
    "    #         self.insert(state, action, reward, next_state, done)\n",
    "\n",
    "    #         # update state\n",
    "    #         if done or truncate:\n",
    "    #             # if truncation reached, reset state\n",
    "    #             state,_ = env.reset()\n",
    "    #         else:\n",
    "    #             # all else state is the next\n",
    "    #             state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1., 1., 1.]), tensor([1., 1., 1.]), tensor([5., 5., 5.]), tensor([2., 2., 2.]), tensor([1., 1., 0.]))\n"
     ]
    }
   ],
   "source": [
    "# TESTING REPLAY BUFFER\n",
    "# test = ReplayBuffer(5)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,False)\n",
    "\n",
    "# sample = test.sample_random_minibatch(3)\n",
    "# print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor AKA: The POLICY\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dims=(400,300), init_weight = 3e3) -> None:\n",
    "        super(Actor, self).__init__()\n",
    "        # In the DDPG paper the parameters for the ACTOR are:\n",
    "        # - Learning rate: 10^-4\n",
    "        # - 2 hidden layers\n",
    "        # - 400 & 300 hidden dims (called units in paper) for first and second hidden layer, respectively\n",
    "        # - ReLU (rectified nonlinearity) for all hidden layers\n",
    "        # - output layer uses tanh (returns actions needed for the agent)\n",
    "\n",
    "        # initializing layer weights\n",
    "        # - hidden layers weights iniitalized with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # - output layer weights initialized with uniform distribution (-3e-3,3e-3)\n",
    "\n",
    "\n",
    "        self.init_weight_limit = init_weight\n",
    "\n",
    "        # hidden layers\n",
    "        self.hidden1 = nn.Linear(num_states, hidden_dims[0]) # input to hidden\n",
    "        self.hidden2 = nn.Linear(hidden_dims[0], hidden_dims[1]) # hidden to hidden\n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_dims[1], num_actions) # hidden to output\n",
    "        # activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input to first hidden layer w/ relu activation\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # feed into second hidden layer w/ relu activation\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # feed through output layer w/ tanh activation\n",
    "        x = self.output(x)\n",
    "        y = self.tanh(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # init hidden with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        self.hidden1.weight.data.uniform_(-(1/math.sqrt(self.hidden1.weight.size(1))),(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden2.weight.data.uniform_(-(1/math.sqrt(self.hidden2.weight.size(1))),(1/math.sqrt(self.hidden2.weight.size(1))))\n",
    "        # output layer weights init with uniform distribution (-3e-3,3e-3)\n",
    "        self.output.weight.data.uniform_(-self.init_weight_limit, self.init_weight_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic AKA: The Q-VALUE FUNCTION\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, output_dim=1, hidden_dims=(400,300), init_weight = 3e3) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        # In the DDPG paper the parameters for the CRITIC are:\n",
    "        # - Learning rate: 10^-3\n",
    "        # - 2 hidden layers\n",
    "        # - 400 & 300 hidden dims (called units in paper) for first and second hidden layer, respectively\n",
    "        # - ReLU (rectified nonlinearity) for all hidden layers\n",
    "        # - output layer uses tanh (returns a single q-value for the input state-action pair)\n",
    "        # - output layer weights initialized with uniform distribution (low=-3e-3,high=3e-3)\n",
    "\n",
    "        # initializing layer weights\n",
    "        # - hidden layers weights iniitalized with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # - output layer weights initialized with uniform distribution (-3e-3,3e-3)\n",
    "\n",
    "        self.init_weight_limit = init_weight\n",
    "\n",
    "        # hidden layers\n",
    "        self.hidden1 = nn.Linear(num_states, hidden_dims[0]) # input to hidden, nn.Linear are the next layers after the given input x\n",
    "        self.hidden2 = nn.Linear(hidden_dims[0]+num_actions, hidden_dims[1]) # hidden to hidden\n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_dims[1], output_dim) # hidden to output\n",
    "        # activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pull state and action from input\n",
    "        state, action = x\n",
    "\n",
    "        # first hidden layer and relu activation\n",
    "        x = self.hidden1(state)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # in critic (Q-value fn) network, the actions are not included until the second hidden layer\n",
    "        # feed thru w/ relu activation\n",
    "        x = self.hidden2(torch.cat([x,action],1))\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # feed through output layer w/ tanh activation\n",
    "        x = self.output(x)\n",
    "        y = self.tanh(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # init hidden with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # alternative method: nn.init.uniform_(self.hidden1.weight, a=-(1/math.sqrt(self.hidden1.weight.size(1))), b=(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden1.weight.data.uniform_(-(1/math.sqrt(self.hidden1.weight.size(1))),(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden2.weight.data.uniform_(-(1/math.sqrt(self.hidden2.weight.size(1))),(1/math.sqrt(self.hidden2.weight.size(1))))\n",
    "        # output layer weights init with uniform distribution (-3e-3,3e-3)\n",
    "        self.output.weight.data.uniform_(-self.init_weight_limit, self.init_weight_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [reference] https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-bipedal/ddpg_agent.py\n",
    "# Used Udacity tutorial for OU noise generation\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, env, params, random_seed) -> None:\n",
    "        # grabbing parameters\n",
    "        self.gamma = params['gamma']\n",
    "        self.tau = params['tau']\n",
    "        self.actor_lr = params['actor_lr']\n",
    "        self.critic_lr = params['critic_lr']\n",
    "        self.batch_size = params['minibatch_size']\n",
    "        self.buffer_size = params['replay_buffer_size']\n",
    "\n",
    "        # setting random seed\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # setting number of states and actions\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.shape[0]\n",
    "\n",
    "        # initialize critic network w target\n",
    "        self.critic = Critic(self.num_states, self.num_actions)\n",
    "        # creating deepcopy to copy the network over to a target\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        # define optimizer\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "\n",
    "        # initialize actor network w target\n",
    "        self.actor = Actor(self.num_states, self.num_actions)\n",
    "        # creating deepcopy to copy the network over to a target\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        # define optimizer\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "\n",
    "        # OU noise for action selection\n",
    "        self.noise = OUNoise(self.num_actions, random_seed)\n",
    "\n",
    "        # initialize replay buffer and prepopulate\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "        # self.replay_buffer.prepopulate()\n",
    "\n",
    "    # get action with some noise\n",
    "    def get_action(self, state):\n",
    "        # convert to tensor to feed into network\n",
    "        state = torch.tensor(state, dtype=torch.double)\n",
    "\n",
    "        # set to eval mode to not track batch norm\n",
    "        self.actor.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state) + self.noise.sample()\n",
    "        self.actor.train()\n",
    "        return action\n",
    "\n",
    "    # updates critic and actor\n",
    "    def update(self):\n",
    "        # sample batch\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, dones_batch = self.replay_buffer.sample_random_minibatch(self.batch_size)\n",
    "\n",
    "        # calculate target batch\n",
    "        with torch.no_grad():\n",
    "            target = self.calculate_target(reward_batch, next_state_batch)\n",
    "\n",
    "        # calculate q-value batch\n",
    "        q_val_batch = self.critic((state_batch, action_batch))\n",
    "\n",
    "        # update critic by minimizing loss\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        loss_val = loss(q_val_batch, target)\n",
    "        loss_val.backward()        \n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # using critic to update actor\n",
    "        loss_actor = -self.critic((state_batch, self.actor(state_batch))) # TODO: should this be negative?\n",
    "        self.actor.zero_grad()\n",
    "        loss_actor = torch.mean(loss_actor)\n",
    "        loss_actor.backwards()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # update target network weights\n",
    "        # update target critic\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )\n",
    "        \n",
    "        # update target actor\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )\n",
    "\n",
    "    def calculate_target(self, reward, next_state):\n",
    "        next_action = self.actor_target_target(next_state)\n",
    "        target = reward + self.gamma*self.critic_target(next_state,next_action)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;66;03m# reset env if done\u001b[39;00m\n\u001b[0;32m     42\u001b[0m         env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 44\u001b[0m \u001b[43mrun_DDPG\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[122], line 29\u001b[0m, in \u001b[0;36mrun_DDPG\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# while environment isnt terminal\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m done:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# grab action with OU noise\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mddpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# execute action in env\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     next_state, reward, done, truncate, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[121], line 48\u001b[0m, in \u001b[0;36mDDPGAgent.get_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 48\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[93], line 33\u001b[0m, in \u001b[0;36mActor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# input to first hidden layer w/ relu activation\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# feed into second hidden layer w/ relu activation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "# TODO: choose device to run on\n",
    "# main loop\n",
    "def run_DDPG():\n",
    "    # initialize parameters\n",
    "    params = {'actor_lr': 0.0001,\n",
    "            'critic_lr': 0.001,\n",
    "            'tau': 0.001,\n",
    "            'gamma': 0.99,\n",
    "            'minibatch_size': 64,\n",
    "            'replay_buffer_size': int(10e6),\n",
    "            'steps': 100_000}\n",
    "    \n",
    "    # create environment\n",
    "    env = gym.make('Hopper-v5')\n",
    "\n",
    "    # ddpg object\n",
    "    ddpg = DDPGAgent(env, params, random_seed=10)\n",
    "\n",
    "    # loop through desired number of steps\n",
    "    for _ in tqdm.tqdm(range(params['steps']), desc=\"steps\"):\n",
    "        # reset env\n",
    "        state,_ = env.reset()\n",
    "\n",
    "        # initialize terminal state\n",
    "        done = True\n",
    "\n",
    "        # while environment isnt terminal\n",
    "        while done:\n",
    "            # grab action with OU noise\n",
    "            action = ddpg.get_action(state)\n",
    "            # execute action in env\n",
    "            next_state, reward, done, truncate, _ = env.step(action)\n",
    "            # store in buffer\n",
    "            ddpg.replay_buffer.insert(state, action, reward, next_state, done)\n",
    "            # learn when buffer reaches batch size\n",
    "            if len(ddpg.replay_buffer.buffer) == ddpg.buffer_size:\n",
    "                ddpg.update()\n",
    "\n",
    "            # update state\n",
    "            state = next_state\n",
    "        \n",
    "        # reset env if done\n",
    "        env.reset()\n",
    "\n",
    "run_DDPG()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Hopper-v5')\n",
    "# env.step()\n",
    "print(env.action_space.shape[0])\n",
    "print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
