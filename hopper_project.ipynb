{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the correct packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for google colab\n",
    "!pip install mujoco\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mujoco\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "# for copying deep nets to another variable\n",
    "from copy import deepcopy\n",
    "\n",
    "# queue for replay buffer\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        # initialize parameters\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def insert(self, obs, action, reward, next_obs, done):\n",
    "        # tuple to represent transition\n",
    "        trans = (torch.tensor(obs, dtype=torch.float32), torch.tensor(action, dtype=torch.float32),\n",
    "                 torch.tensor(reward, dtype=torch.float32), torch.tensor(next_obs, dtype=torch.float32), torch.tensor(done, dtype=torch.float32))\n",
    "\n",
    "        # save transition to buffer\n",
    "        # use deque because once its full it discards old items\n",
    "        self.buffer.append(trans)\n",
    "\n",
    "    def sample_random_minibatch(self, batch_size, device):\n",
    "        # Random idx to sample from buffer w/o replacement\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Unpack batch into separate lists of tensors\n",
    "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
    "        \n",
    "        # Convert lists of tensors into single tensors\n",
    "        obs = torch.stack(obs).to(device)\n",
    "        actions = torch.stack(actions).to(device)\n",
    "        rewards = torch.stack(rewards).to(device)\n",
    "        next_obs = torch.stack(next_obs).to(device)\n",
    "        dones = torch.stack(dones).to(device)\n",
    "\n",
    "        # print(\"state shape\",obs.shape)\n",
    "        # print(\"act shape\",actions.shape)\n",
    "        # print(\"rewards shape\",rewards.shape)\n",
    "        # print(\"next_obs shape\",next_obs.shape)\n",
    "        # print(\"done\", dones.shape)\n",
    "\n",
    "        # tuple of tensors\n",
    "        batch = (obs, actions, rewards, next_obs, dones)\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    # def prepopulate(self, env, actor):\n",
    "    #     # select action from actor\n",
    "    #     # execute action in the env\n",
    "    #     # store transition\n",
    "\n",
    "    #     # intialize state\n",
    "    #     state,_ = env.reset()\n",
    "\n",
    "    #     # loop through num_steps\n",
    "    #     for i in range(self.buffer_size):\n",
    "    #         # choose random action from environment action space (random policy)\n",
    "    #         action = env.action_space.sample()\n",
    "    #         # take action: get next state, reward, done\n",
    "    #         next_state, reward, done, truncate,_ = env.step(action)\n",
    "    #         # add transition to memory\n",
    "    #         self.insert(state, action, reward, next_state, done)\n",
    "\n",
    "    #         # update state\n",
    "    #         if done or truncate:\n",
    "    #             # if truncation reached, reset state\n",
    "    #             state,_ = env.reset()\n",
    "    #         else:\n",
    "    #             # all else state is the next\n",
    "    #             state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING REPLAY BUFFER\n",
    "# test = ReplayBuffer(5)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,False)\n",
    "\n",
    "# sample = test.sample_random_minibatch(3)\n",
    "# print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor AKA: The POLICY\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dims=(400,300), init_weight = 3e-3) -> None:\n",
    "        super(Actor, self).__init__()\n",
    "        # In the DDPG paper the parameters for the ACTOR are:\n",
    "        # - Learning rate: 10^-4\n",
    "        # - 2 hidden layers\n",
    "        # - 400 & 300 hidden dims (called units in paper) for first and second hidden layer, respectively\n",
    "        # - ReLU (rectified nonlinearity) for all hidden layers\n",
    "        # - output layer uses tanh (returns actions needed for the agent)\n",
    "\n",
    "        # initializing layer weights\n",
    "        # - hidden layers weights iniitalized with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # - output layer weights initialized with uniform distribution (-3e-3,3e-3)\n",
    "        self.init_weight_limit = init_weight\n",
    "\n",
    "        # hidden layers\n",
    "        self.hidden1 = nn.Linear(num_states, hidden_dims[0]) # input to hidden\n",
    "        self.hidden2 = nn.Linear(hidden_dims[0], hidden_dims[1]) # hidden to hidden\n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_dims[1], num_actions) # hidden to output\n",
    "        # activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input to first hidden layer w/ relu activation\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # feed into second hidden layer w/ relu activation\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # feed through output layer w/ tanh activation\n",
    "        x = self.output(x)\n",
    "        y = self.tanh(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # init hidden with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        self.hidden1.weight.data.uniform_(-(1/math.sqrt(self.hidden1.weight.size(1))),(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden2.weight.data.uniform_(-(1/math.sqrt(self.hidden2.weight.size(1))),(1/math.sqrt(self.hidden2.weight.size(1))))\n",
    "        # output layer weights init with uniform distribution (-3e-3,3e-3)\n",
    "        self.output.weight.data.uniform_(-self.init_weight_limit, self.init_weight_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic AKA: The Q-VALUE FUNCTION\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, output_dim=1, hidden_dims=(400,300), init_weight = 3e-3) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        # In the DDPG paper the parameters for the CRITIC are:\n",
    "        # - Learning rate: 10^-3\n",
    "        # - 2 hidden layers\n",
    "        # - 400 & 300 hidden dims (called units in paper) for first and second hidden layer, respectively\n",
    "        # - ReLU (rectified nonlinearity) for all hidden layers\n",
    "        # - output layer uses tanh (returns a single q-value for the input state-action pair)\n",
    "        # - output layer weights initialized with uniform distribution (low=-3e-3,high=3e-3)\n",
    "\n",
    "        # initializing layer weights\n",
    "        # - hidden layers weights iniitalized with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # - output layer weights initialized with uniform distribution (-3e-3,3e-3)\n",
    "        self.init_weight_limit = init_weight\n",
    "\n",
    "        # hidden layers\n",
    "        self.hidden1 = nn.Linear(num_states, hidden_dims[0]) # input to hidden, nn.Linear are the next layers after the given input x\n",
    "        self.hidden2 = nn.Linear(hidden_dims[0]+num_actions, hidden_dims[1]) # hidden to hidden\n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_dims[1], output_dim) # hidden to output\n",
    "        # activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # first hidden layer and relu activation\n",
    "        x = self.hidden1(state)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # in critic (Q-value fn) network, the actions are not included until the second hidden layer\n",
    "        # feed thru w/ relu activation\n",
    "        x = self.hidden2(torch.cat([x,action],1))\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # feed through output layer\n",
    "        y = self.output(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # init hidden with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # alternative method: nn.init.uniform_(self.hidden1.weight, a=-(1/math.sqrt(self.hidden1.weight.size(1))), b=(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden1.weight.data.uniform_(-(1/math.sqrt(self.hidden1.weight.size(1))),(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden2.weight.data.uniform_(-(1/math.sqrt(self.hidden2.weight.size(1))),(1/math.sqrt(self.hidden2.weight.size(1))))\n",
    "        # output layer weights init with uniform distribution (-3e-3,3e-3)\n",
    "        self.output.weight.data.uniform_(-self.init_weight_limit, self.init_weight_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [reference] https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-bipedal/ddpg_agent.py\n",
    "# Used Udacity tutorial for OU noise generation\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return torch.tensor(self.state, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, env, params, random_seed) -> None:\n",
    "        # grabbing parameters\n",
    "        self.gamma = params['gamma']\n",
    "        self.tau = params['tau']\n",
    "        self.actor_lr = params['actor_lr']\n",
    "        self.critic_lr = params['critic_lr']\n",
    "        self.batch_size = params['minibatch_size']\n",
    "        self.buffer_size = params['replay_buffer_size']\n",
    "        self.weight_decay = params['L2_weight_decay']\n",
    "\n",
    "        # setting random seeds\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed) # setting for cuda if GPU available\n",
    "\n",
    "        # setting number of states and actions\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.shape[0]\n",
    "\n",
    "        # choose device\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        # initialize critic network w target\n",
    "        self.critic = Critic(self.num_states, self.num_actions).to(self.device)\n",
    "        # summary(self.critic, input_size=(2))\n",
    "        # creating deepcopy to copy the network over to a target\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        # define optimizer\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.critic_lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        # initialize actor network w target\n",
    "        self.actor = Actor(self.num_states, self.num_actions).to(self.device)\n",
    "        # summary(self.actor, input_size=(11,))\n",
    "        # creating deepcopy to copy the network over to a target\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        # define optimizer\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "\n",
    "        # OU noise for action selection\n",
    "        self.noise = OUNoise(self.num_actions, random_seed)\n",
    "\n",
    "        # initialize replay buffer and prepopulate\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "        # self.replay_buffer.prepopulate()\n",
    "\n",
    "    # get action with some noise\n",
    "    def get_action(self, env, state):\n",
    "        # convert to tensor to feed into network\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # set to eval mode to not track batch norm\n",
    "        self.actor.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "            action += self.noise.sample().to(self.device)\n",
    "        self.actor.train()\n",
    "        \n",
    "        # copy to cpu if necessary\n",
    "        # convert to numpy for OpenAI step input\n",
    "        action = action.cpu().numpy()\n",
    "        action = np.clip(action,env.action_space.low,env.action_space.high)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    # updates critic and actor\n",
    "    def update(self):\n",
    "        # sample batch\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, dones_batch = self.replay_buffer.sample_random_minibatch(self.batch_size, self.device)\n",
    "\n",
    "        # calculate target batch\n",
    "        # with torch.no_grad():\n",
    "        target = self.calculate_target(reward_batch, next_state_batch, dones_batch)\n",
    "\n",
    "        # calculate q-value batch\n",
    "        q_val_batch = self.critic(state_batch, action_batch)\n",
    "\n",
    "        # updating critic: by minimizing loss\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        loss_val = loss(q_val_batch, target)\n",
    "        loss_val.backward()        \n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # updating actor: using critic to update actor\n",
    "        loss_actor = -self.critic(state_batch, self.actor(state_batch)) # TODO: should this be negative?\n",
    "        \n",
    "        self.actor.zero_grad()\n",
    "        loss_actor = torch.mean(loss_actor)\n",
    "        loss_actor.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # update target network weights\n",
    "        # update target critic\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )\n",
    "        \n",
    "        # update target actor\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )\n",
    "\n",
    "        return loss_actor.item()\n",
    "\n",
    "    def calculate_target(self, reward, next_state, dones):\n",
    "        next_action = self.actor_target(next_state)\n",
    "        target = reward.view(-1,1) + self.gamma*self.critic_target(next_state, next_action)*(1-dones.view(-1,1))\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Step 2245 \tEpisode 100\tAverage Score: 12.46ore: 12.46 | Episode Score: 3.36\n",
      "Step 4818 \tEpisode 200\tAverage Score: 4.68core: 4.68 | Episode Score: 4.903\n",
      "Step 7826 \tEpisode 300\tAverage Score: 4.12core: 4.12 | Episode Score: 2.89\n",
      "Step 10514 \tEpisode 400\tAverage Score: 14.99ore: 14.99 | Episode Score: 37.58\n",
      "Step 12723 \tEpisode 500\tAverage Score: 37.86ore: 37.86 | Episode Score: 39.16\n",
      "Step 14931 \tEpisode 600\tAverage Score: 37.84ore: 37.84 | Episode Score: 37.86\n",
      "Step 17156 \tEpisode 700\tAverage Score: 38.09ore: 38.09 | Episode Score: 39.01\n",
      "Step 20285 \tEpisode 800\tAverage Score: 8.93core: 8.93 | Episode Score: 3.37300\n",
      "Step 23240 \tEpisode 900\tAverage Score: 3.87core: 3.87 | Episode Score: 5.59\n",
      "Step 26190 \tEpisode 1000\tAverage Score: 3.79core: 3.79 | Episode Score: 4.04\n",
      "Step 30596 \tEpisode 1100\tAverage Score: 42.07ore: 42.07 | Episode Score: 66.790\n",
      "Step 33295 \tEpisode 1200\tAverage Score: 36.98ore: 36.98 | Episode Score: 38.01\n",
      "Step 35506 \tEpisode 1300\tAverage Score: 37.87ore: 37.87 | Episode Score: 37.58\n",
      "Step 37721 \tEpisode 1400\tAverage Score: 38.02ore: 38.02 | Episode Score: 37.70\n",
      "Step 39924 \tEpisode 1500\tAverage Score: 37.79ore: 37.79 | Episode Score: 39.27\n",
      "Step 42141 \tEpisode 1600\tAverage Score: 37.98ore: 37.98 | Episode Score: 37.58\n",
      "Step 44353 \tEpisode 1700\tAverage Score: 37.94ore: 37.94 | Episode Score: 37.92\n",
      "Step 46555 \tEpisode 1800\tAverage Score: 37.77ore: 37.77 | Episode Score: 37.48\n",
      "Step 48758 \tEpisode 1900\tAverage Score: 37.78ore: 37.78 | Episode Score: 37.44\n",
      "Step 50965 \tEpisode 2000\tAverage Score: 37.89ore: 37.89 | Episode Score: 39.47\n",
      "Step 53175 \tEpisode 2100\tAverage Score: 37.88ore: 37.88 | Episode Score: 36.07\n",
      "Step 55386 \tEpisode 2200\tAverage Score: 37.91ore: 37.91 | Episode Score: 37.99\n",
      "Step 57584 \tEpisode 2300\tAverage Score: 37.66ore: 37.66 | Episode Score: 37.97\n",
      "Step 59789 \tEpisode 2400\tAverage Score: 37.77ore: 37.77 | Episode Score: 37.87\n",
      "Step 61990 \tEpisode 2500\tAverage Score: 37.76ore: 37.76 | Episode Score: 38.21\n",
      "Step 64204 \tEpisode 2600\tAverage Score: 37.94ore: 37.94 | Episode Score: 39.21\n",
      "Step 66402 \tEpisode 2700\tAverage Score: 37.69ore: 37.69 | Episode Score: 39.28\n",
      "Step 68608 \tEpisode 2800\tAverage Score: 37.81ore: 37.81 | Episode Score: 37.71\n",
      "Step 70821 \tEpisode 2900\tAverage Score: 37.87ore: 37.87 | Episode Score: 37.62\n",
      "Step 73036 \tEpisode 3000\tAverage Score: 37.97ore: 37.97 | Episode Score: 37.88\n",
      "Step 75241 \tEpisode 3100\tAverage Score: 37.79ore: 37.79 | Episode Score: 35.74\n",
      "Step 77451 \tEpisode 3200\tAverage Score: 37.89ore: 37.89 | Episode Score: 37.90\n",
      "Step 79662 \tEpisode 3300\tAverage Score: 37.85ore: 37.85 | Episode Score: 39.40\n",
      "Step 81879 \tEpisode 3400\tAverage Score: 37.99ore: 37.99 | Episode Score: 36.23\n",
      "Step 84089 \tEpisode 3500\tAverage Score: 37.87ore: 37.87 | Episode Score: 37.50\n",
      "Step 86298 \tEpisode 3600\tAverage Score: 37.85ore: 37.85 | Episode Score: 36.08\n",
      "Step 88502 \tEpisode 3700\tAverage Score: 37.75ore: 37.75 | Episode Score: 37.58\n",
      "Step 90715 \tEpisode 3800\tAverage Score: 37.93ore: 37.93 | Episode Score: 37.67\n",
      "Step 92923 \tEpisode 3900\tAverage Score: 37.82ore: 37.82 | Episode Score: 39.47\n",
      "Step 95131 \tEpisode 4000\tAverage Score: 37.83ore: 37.83 | Episode Score: 36.39\n",
      "Step 97348 \tEpisode 4100\tAverage Score: 38.01ore: 38.01 | Episode Score: 37.57\n",
      "Step 99565 \tEpisode 4200\tAverage Score: 38.01ore: 38.01 | Episode Score: 37.93\n",
      "Step 100008/100000 | Episode 4220 | Average Score: 37.93 | Episode Score: 39.42Using cpu device\n",
      "Step 12000/100000 | Episode 12 | Average Score: -174.46 | Episode Score: -164.16"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m render_mode \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, env_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(environments):\n\u001b[1;32m---> 98\u001b[0m     scores, loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_DDPG\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(scores)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), scores)\n\u001b[0;32m    101\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCumulative reward for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[45], line 56\u001b[0m, in \u001b[0;36mrun_DDPG\u001b[1;34m(env_type, mode)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# learn when buffer reaches batch size\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ddpg\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m ddpg\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m---> 56\u001b[0m     loss_item \u001b[38;5;241m=\u001b[39m \u001b[43mddpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     loss\u001b[38;5;241m.\u001b[39mappend(loss_item)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# update state\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 100\u001b[0m, in \u001b[0;36mDDPGAgent.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     99\u001b[0m loss_actor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(loss_actor)\n\u001b[1;32m--> 100\u001b[0m \u001b[43mloss_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# update target network weights\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# update target critic\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# main loop\n",
    "def run_DDPG(env_type, mode):\n",
    "    # initialize parameters\n",
    "    params = {'actor_lr': 0.0001,\n",
    "            'critic_lr': 0.001,\n",
    "            'tau': 0.0001,\n",
    "            'gamma': 0.99,\n",
    "            'minibatch_size': 64,\n",
    "            'replay_buffer_size': int(1e6),\n",
    "            'steps': 100_000,\n",
    "            'L2_weight_decay': 1e-2}\n",
    "\n",
    "    # grab cwd for model saving\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # create environment\n",
    "    # env = gym.make(env_type, render_mode=mode)\n",
    "    env = gym.make(env_type)\n",
    "\n",
    "    # ddpg object\n",
    "    ddpg = DDPGAgent(env, params, random_seed=10)\n",
    "\n",
    "    # keep track of loss\n",
    "    scores = []\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    loss = []\n",
    "    ep_count = 0\n",
    "    step_count = 0\n",
    "\n",
    "    # loop through desired number of steps\n",
    "    while step_count < params['steps']:\n",
    "        # reset env\n",
    "        state,_ = env.reset()\n",
    "\n",
    "        # initialize terminal state\n",
    "        done = False\n",
    "        truncate = False\n",
    "\n",
    "        # track cumulative reward\n",
    "        cumulative_reward = 0\n",
    "\n",
    "        # while environment isnt terminal\n",
    "        while not (done or truncate):\n",
    "            # grab action with OU noise\n",
    "            action = ddpg.get_action(env, state)\n",
    "            # execute action in env\n",
    "            next_state, reward, done, truncate, _ = env.step(action)\n",
    "            # increment step\n",
    "            step_count += 1\n",
    "            \n",
    "            # store in buffer\n",
    "            ddpg.replay_buffer.insert(state, action, reward, next_state, done)\n",
    "\n",
    "            # learn when buffer reaches batch size\n",
    "            if len(ddpg.replay_buffer.buffer) > ddpg.batch_size:\n",
    "                loss_item = ddpg.update()\n",
    "                loss.append(loss_item)\n",
    "\n",
    "            # update state\n",
    "            state = next_state\n",
    "\n",
    "            # update cumulative reward\n",
    "            cumulative_reward += reward\n",
    "\n",
    "        # add to ep count\n",
    "        ep_count += 1\n",
    "\n",
    "        # append to running score and to score deque for average reward approximation\n",
    "        scores.append(cumulative_reward)\n",
    "        scores_deque.append(cumulative_reward)\n",
    "        print(f\"\\rStep {step_count}/{params['steps']} | Episode {ep_count} | \"\n",
    "          f\"Average Score: {np.mean(scores_deque):.2f} | \"\n",
    "          f\"Episode Score: {cumulative_reward:.2f}\", end=\"\", flush=True)\n",
    "        # print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpisode score: {:.2f}'.format(ep, np.mean(scores_deque), cumulative_reward), end=\"\")\n",
    "        # save scores\n",
    "        if ep_count % 100 == 0:\n",
    "            torch.save(ddpg.actor.state_dict(), f'{cwd}/checkpoint_{env_type}_actor.pth')\n",
    "            torch.save(ddpg.critic.state_dict(), f'{cwd}/checkpoint_{env_type}_critic.pth')\n",
    "            print('\\rStep {} \\tEpisode {}\\tAverage Score: {:.2f}'.format(step_count, ep_count, np.mean(scores_deque)))   \n",
    "        \n",
    "        # reset env if done\n",
    "        env.reset()\n",
    "\n",
    "    # save scores and loss\n",
    "    scores_df = pd.DataFrame({\"Episode\": np.arange(1, len(scores) + 1), \"Scores\": scores})\n",
    "    loss_df = pd.DataFrame({\"Step\": np.arange(1, len(loss) + 1), \"Loss\": loss})\n",
    "\n",
    "    scores_df.to_csv(f\"{cwd}/results_{env_type}_scores.csv\", index=False)\n",
    "    loss_df.to_csv(f\"{cwd}/results_{env_type}_loss.csv\", index=False)\n",
    "    \n",
    "    return scores, loss\n",
    "\n",
    "environments = ['Hopper-v5', 'HalfCheetah-v5', 'BipedalWalker-v3']\n",
    "\n",
    "render_mode = ['human','human','human','human']\n",
    "\n",
    "for i, env_type in enumerate(environments):\n",
    "    scores, loss = run_DDPG(env_type, render_mode[i])\n",
    "\n",
    "    plt.plot(np.arange(1,len(scores)+1), scores)\n",
    "    plt.title(f\"Cumulative reward for {env_type}\")\n",
    "    plt.ylabel('Cumulative reward')\n",
    "    plt.xlabel('Episodes')\n",
    "    plot_file = f\"{os.getcwd()}/plot_{env_type}_scores.png\"\n",
    "    plt.savefig(plot_file)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# grab cwd for model saving\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# instantiate env\n",
    "env = gym.make('Hopper-v5', render_mode='human')\n",
    "\n",
    "# initialize parameters\n",
    "params = {'actor_lr': 0.0001,\n",
    "        'critic_lr': 0.001,\n",
    "        'tau': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'minibatch_size': 64,\n",
    "        'replay_buffer_size': int(10e6),\n",
    "        'steps': 100_000}\n",
    "\n",
    "ddpg = DDPGAgent(env, params, random_seed=10)\n",
    "\n",
    "ddpg.actor.load_state_dict(torch.load(cwd+'/checkpoint_actor.pth'))\n",
    "ddpg.critic.load_state_dict(torch.load(cwd+'/checkpoint_critic.pth'))\n",
    "\n",
    "state,_ = env.reset()  \n",
    "while True:\n",
    "    action = ddpg.get_action(env, state)\n",
    "    env.render()\n",
    "    next_state, reward, done, truncate, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
