{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the correct packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mujoco\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import tqdm\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "# for copying deep nets to another variable\n",
    "from copy import deepcopy\n",
    "\n",
    "# library for ou noise as implemented with the paper\n",
    "from ou_noise import ou\n",
    "\n",
    "# to view model summary\n",
    "from torchsummary import summary\n",
    "\n",
    "# queue for replay buffer\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        # initialize parameters\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def insert(self, obs, action, reward, next_obs, done):\n",
    "        # tuple to represent transition\n",
    "        trans = (torch.tensor(obs, dtype=torch.float32), torch.tensor(action, dtype=torch.float32),\n",
    "                 torch.tensor(reward, dtype=torch.float32), torch.tensor(next_obs, dtype=torch.float32), torch.tensor(done, dtype=torch.float32))\n",
    "\n",
    "        # save transition to buffer\n",
    "        # use deque because once its full it discards old items\n",
    "        self.buffer.append(trans)\n",
    "\n",
    "    def sample_random_minibatch(self, batch_size, device):\n",
    "        # Random idx to sample from buffer w/o replacement\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Unpack batch into separate lists of tensors\n",
    "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
    "        \n",
    "        # Convert lists of tensors into single tensors\n",
    "        obs = torch.stack(obs).to(device)\n",
    "        actions = torch.stack(actions).to(device)\n",
    "        rewards = torch.stack(rewards).to(device)\n",
    "        next_obs = torch.stack(next_obs).to(device)\n",
    "        dones = torch.stack(dones).to(device)\n",
    "\n",
    "        # # convert list to tensor for easy slciing\n",
    "        # batch = torch.tensor(batch)\n",
    "\n",
    "        # # slicing to grab elements\n",
    "        # obs = batch[:,0]\n",
    "        # actions =  batch[:,1]\n",
    "        # rewards = batch[:,2]\n",
    "        # next_obs = batch[:,3]\n",
    "        # dones = batch[:,4]\n",
    "\n",
    "        # tuple of tensors\n",
    "        batch = (obs, actions, rewards, next_obs, dones)\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    # def prepopulate(self, env, actor):\n",
    "    #     # select action from actor\n",
    "    #     # execute action in the env\n",
    "    #     # store transition\n",
    "\n",
    "    #     # intialize state\n",
    "    #     state,_ = env.reset()\n",
    "\n",
    "    #     # loop through num_steps\n",
    "    #     for i in range(self.buffer_size):\n",
    "    #         # choose random action from environment action space (random policy)\n",
    "    #         action = env.action_space.sample()\n",
    "    #         # take action: get next state, reward, done\n",
    "    #         next_state, reward, done, truncate,_ = env.step(action)\n",
    "    #         # add transition to memory\n",
    "    #         self.insert(state, action, reward, next_state, done)\n",
    "\n",
    "    #         # update state\n",
    "    #         if done or truncate:\n",
    "    #             # if truncation reached, reset state\n",
    "    #             state,_ = env.reset()\n",
    "    #         else:\n",
    "    #             # all else state is the next\n",
    "    #             state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING REPLAY BUFFER\n",
    "# test = ReplayBuffer(5)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,True)\n",
    "# test.insert(1,1.0,5,2,False)\n",
    "\n",
    "# sample = test.sample_random_minibatch(3)\n",
    "# print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor AKA: The POLICY\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dims=(400,300), init_weight = 3e3) -> None:\n",
    "        super(Actor, self).__init__()\n",
    "        # In the DDPG paper the parameters for the ACTOR are:\n",
    "        # - Learning rate: 10^-4\n",
    "        # - 2 hidden layers\n",
    "        # - 400 & 300 hidden dims (called units in paper) for first and second hidden layer, respectively\n",
    "        # - ReLU (rectified nonlinearity) for all hidden layers\n",
    "        # - output layer uses tanh (returns actions needed for the agent)\n",
    "\n",
    "        # initializing layer weights\n",
    "        # - hidden layers weights iniitalized with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # - output layer weights initialized with uniform distribution (-3e-3,3e-3)\n",
    "\n",
    "\n",
    "        self.init_weight_limit = init_weight\n",
    "\n",
    "        # hidden layers\n",
    "        self.hidden1 = nn.Linear(num_states, hidden_dims[0]) # input to hidden\n",
    "        self.hidden2 = nn.Linear(hidden_dims[0], hidden_dims[1]) # hidden to hidden\n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_dims[1], num_actions) # hidden to output\n",
    "        # activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input to first hidden layer w/ relu activation\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # feed into second hidden layer w/ relu activation\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # feed through output layer w/ tanh activation\n",
    "        x = self.output(x)\n",
    "        y = self.tanh(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # init hidden with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        self.hidden1.weight.data.uniform_(-(1/math.sqrt(self.hidden1.weight.size(1))),(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden2.weight.data.uniform_(-(1/math.sqrt(self.hidden2.weight.size(1))),(1/math.sqrt(self.hidden2.weight.size(1))))\n",
    "        # output layer weights init with uniform distribution (-3e-3,3e-3)\n",
    "        self.output.weight.data.uniform_(-self.init_weight_limit, self.init_weight_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic AKA: The Q-VALUE FUNCTION\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, output_dim=1, hidden_dims=(400,300), init_weight = 3e3) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        # In the DDPG paper the parameters for the CRITIC are:\n",
    "        # - Learning rate: 10^-3\n",
    "        # - 2 hidden layers\n",
    "        # - 400 & 300 hidden dims (called units in paper) for first and second hidden layer, respectively\n",
    "        # - ReLU (rectified nonlinearity) for all hidden layers\n",
    "        # - output layer uses tanh (returns a single q-value for the input state-action pair)\n",
    "        # - output layer weights initialized with uniform distribution (low=-3e-3,high=3e-3)\n",
    "\n",
    "        # initializing layer weights\n",
    "        # - hidden layers weights iniitalized with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # - output layer weights initialized with uniform distribution (-3e-3,3e-3)\n",
    "\n",
    "        self.init_weight_limit = init_weight\n",
    "\n",
    "        # hidden layers\n",
    "        self.hidden1 = nn.Linear(num_states, hidden_dims[0]) # input to hidden, nn.Linear are the next layers after the given input x\n",
    "        self.hidden2 = nn.Linear(hidden_dims[0]+num_actions, hidden_dims[1]) # hidden to hidden\n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_dims[1], output_dim) # hidden to output\n",
    "        # activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pull state and action from input\n",
    "        state, action = x\n",
    "\n",
    "        # first hidden layer and relu activation\n",
    "        x = self.hidden1(state)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # in critic (Q-value fn) network, the actions are not included until the second hidden layer\n",
    "        # feed thru w/ relu activation\n",
    "        x = self.hidden2(torch.cat([x,action],1))\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # feed through output layer w/ tanh activation\n",
    "        x = self.output(x)\n",
    "        y = self.tanh(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # init hidden with uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)); fan_in being the input of that particular layer\n",
    "        # alternative method: nn.init.uniform_(self.hidden1.weight, a=-(1/math.sqrt(self.hidden1.weight.size(1))), b=(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden1.weight.data.uniform_(-(1/math.sqrt(self.hidden1.weight.size(1))),(1/math.sqrt(self.hidden1.weight.size(1))))\n",
    "        self.hidden2.weight.data.uniform_(-(1/math.sqrt(self.hidden2.weight.size(1))),(1/math.sqrt(self.hidden2.weight.size(1))))\n",
    "        # output layer weights init with uniform distribution (-3e-3,3e-3)\n",
    "        self.output.weight.data.uniform_(-self.init_weight_limit, self.init_weight_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [reference] https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-bipedal/ddpg_agent.py\n",
    "# Used Udacity tutorial for OU noise generation\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return torch.tensor(self.state, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, env, params, random_seed) -> None:\n",
    "        # grabbing parameters\n",
    "        self.gamma = params['gamma']\n",
    "        self.tau = params['tau']\n",
    "        self.actor_lr = params['actor_lr']\n",
    "        self.critic_lr = params['critic_lr']\n",
    "        self.batch_size = params['minibatch_size']\n",
    "        self.buffer_size = params['replay_buffer_size']\n",
    "\n",
    "        # setting random seed\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # setting number of states and actions\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.shape[0]\n",
    "\n",
    "        # choose device\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        # initialize critic network w target\n",
    "        self.critic = Critic(self.num_states, self.num_actions).to(self.device)\n",
    "        # summary(self.critic, input_size=(2))\n",
    "        # creating deepcopy to copy the network over to a target\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        # define optimizer\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "\n",
    "        # initialize actor network w target\n",
    "        self.actor = Actor(self.num_states, self.num_actions).to(self.device)\n",
    "        # summary(self.actor, input_size=(11,))\n",
    "        # creating deepcopy to copy the network over to a target\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        # define optimizer\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "\n",
    "        # OU noise for action selection\n",
    "        self.noise = OUNoise(self.num_actions, random_seed)\n",
    "\n",
    "        # initialize replay buffer and prepopulate\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "        # self.replay_buffer.prepopulate()\n",
    "\n",
    "    # get action with some noise\n",
    "    def get_action(self, state):\n",
    "        # convert to tensor to feed into network\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # set to eval mode to not track batch norm\n",
    "        self.actor.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "            action += self.noise.sample()\n",
    "        self.actor.train()\n",
    "        \n",
    "        return action.numpy()\n",
    "    \n",
    "    # updates critic and actor\n",
    "    def update(self):\n",
    "        # sample batch\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, dones_batch = self.replay_buffer.sample_random_minibatch(self.batch_size, self.device)\n",
    "\n",
    "        # calculate target batch\n",
    "        with torch.no_grad():\n",
    "            target = self.calculate_target(reward_batch, next_state_batch)\n",
    "\n",
    "        # calculate q-value batch\n",
    "        q_val_batch = self.critic((state_batch, action_batch))\n",
    "\n",
    "        # update critic by minimizing loss\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        loss_val = loss(q_val_batch, target)\n",
    "        loss_val.backward()        \n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # using critic to update actor\n",
    "        loss_actor = -self.critic((state_batch, self.actor(state_batch))) # TODO: should this be negative?\n",
    "        \n",
    "        self.actor.zero_grad()\n",
    "        loss_actor = torch.mean(loss_actor)\n",
    "        loss_actor.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # update target network weights\n",
    "        # update target critic\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )\n",
    "        \n",
    "        # update target actor\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )\n",
    "\n",
    "        return loss_actor.item()\n",
    "\n",
    "    def calculate_target(self, reward, next_state):\n",
    "        next_action = self.actor_target(next_state)\n",
    "        target = reward + self.gamma*self.critic_target((next_state, next_action))\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.24741459e+00  1.30447929e-03 -4.92656566e-03 -2.01105469e-03\n",
      " -1.92940047e-03 -1.78545562e-03  2.63571942e-03 -4.79845063e-03\n",
      " -1.22794558e-03 -3.66053091e-03 -4.49768400e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "steps:   0%|          | 1/100000 [00:00<7:43:28,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: 43.09\n",
      "[ 1.25120947e+00 -4.38383478e-04  1.33654910e-03  3.48032333e-03\n",
      "  1.05385943e-04  1.95775182e-03 -3.02278952e-03  4.81072188e-03\n",
      "  3.21575717e-03 -4.76749248e-03 -3.97170813e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 2/100000 [00:01<16:55:53,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25008076e+00  7.95658283e-04 -1.72570740e-04  1.72518823e-03\n",
      " -1.73637295e-03 -4.96398979e-03  1.29146062e-03 -3.41471980e-04\n",
      " -3.45857167e-04 -1.23128617e-03 -2.06952534e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 3/100000 [00:01<16:00:05,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25112486e+00  9.29236930e-04 -4.16233536e-03  4.12463023e-03\n",
      " -2.67299489e-03  1.16860897e-03 -4.64126375e-03 -2.69559180e-03\n",
      " -2.65172142e-03 -3.65637347e-03 -2.74474008e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 4/100000 [00:02<16:16:14,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.24929492e+00 -1.75693815e-03  1.81950674e-03 -9.52513946e-04\n",
      "  4.04060838e-03 -3.68583759e-03 -2.98789029e-03  1.02043315e-03\n",
      "  3.16876547e-03  4.89361625e-03  2.97000846e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 5/100000 [00:03<19:01:43,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25128424e+00  2.29820107e-03  3.61468411e-03 -7.40537157e-04\n",
      " -7.29177068e-04 -3.15461095e-03  5.07596169e-04  3.59385689e-03\n",
      "  4.24763432e-04  2.64644477e-03 -1.62530329e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 6/100000 [00:04<22:07:45,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25053708e+00  1.37449386e-04 -8.04071235e-04  4.52189303e-03\n",
      "  4.80515436e-03  3.85165671e-03  5.70193836e-04  1.38848416e-03\n",
      " -4.61426268e-03 -3.43437194e-03 -1.92962610e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 7/100000 [00:04<20:20:29,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25310249e+00  3.37176982e-03 -1.13657700e-03  7.03040170e-04\n",
      "  2.86024392e-03  2.61839314e-03 -4.09741336e-04  3.12070183e-03\n",
      "  1.11337728e-03  1.57001179e-03 -3.68992725e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 8/100000 [00:05<16:59:04,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.24711921e+00  1.64918037e-03 -3.39727831e-03 -3.16555009e-03\n",
      "  2.28718996e-03  3.10086720e-04  2.12679539e-03 -2.92036291e-03\n",
      "  2.89923706e-03  2.33714590e-04 -4.50015094e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 9/100000 [00:05<19:35:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25087134e+00 -9.05095140e-04 -3.83651796e-03  1.23659697e-03\n",
      " -3.11301594e-03 -1.32221696e-03 -1.34791276e-04  1.14922832e-03\n",
      "  1.33429802e-03 -8.84357820e-04 -1.01501416e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 10/100000 [00:06<17:31:37,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25050828e+00 -3.41413022e-03 -1.98758726e-03 -8.10437979e-04\n",
      " -8.63284574e-04 -1.26011879e-03  4.64500350e-03 -1.41954314e-03\n",
      " -1.64863349e-04 -1.81600242e-03 -2.32262367e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 11/100000 [00:07<19:13:30,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.24572263e+00  3.95313634e-04  2.15244303e-03  4.16451125e-03\n",
      " -1.91360290e-03 -1.41157000e-03  3.30800008e-04 -1.86824070e-03\n",
      "  1.93920319e-03  3.64083463e-03 -3.03394776e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 12/100000 [00:08<19:28:30,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25440081e+00 -3.78372126e-03  1.68362922e-04 -4.79548624e-03\n",
      "  1.90957420e-03 -4.57937292e-03  3.14908678e-03 -3.61653651e-03\n",
      "  3.80232004e-03  1.40278713e-03  1.08893894e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 13/100000 [00:08<21:22:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.24733964e+00 -1.01430691e-03 -2.36001984e-03 -3.69522718e-04\n",
      " -2.34978528e-04  4.63250799e-03  3.10137337e-03  4.09544628e-04\n",
      "  1.41364731e-03  3.15942659e-03 -2.14530199e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 14/100000 [00:09<22:43:34,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.24577789e+00  4.66714513e-03 -7.85593834e-04 -2.12533789e-03\n",
      " -4.89334169e-04  3.78541397e-03 -3.19381430e-03  1.11000312e-03\n",
      " -3.72353602e-03  8.57812778e-04 -1.27305436e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 15/100000 [00:11<25:37:02,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.24934296e+00 -3.34615185e-03  1.87466469e-03 -3.79202657e-03\n",
      "  5.28321871e-04 -4.63464006e-03 -4.15048083e-03  2.96964616e-03\n",
      " -4.96372480e-03 -1.25364324e-03 -3.06635361e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 16/100000 [00:11<22:04:02,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.24680134e+00 -3.67071391e-03 -3.13281795e-03 -6.72942102e-04\n",
      " -1.39560286e-03  2.82283133e-03  4.52942585e-03 -8.39546117e-04\n",
      "  2.12055366e-03 -4.79089033e-03 -1.10745060e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 17/100000 [00:12<23:06:07,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25109252e+00  2.04211236e-03  2.71203303e-03 -4.01809607e-04\n",
      "  3.01540325e-03 -2.02564967e-03 -4.34757062e-03  3.99504941e-03\n",
      "  6.40905755e-04  2.68145601e-03 -1.54896481e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 17/100000 [00:13<21:34:24,  1.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[349], line 75\u001b[0m\n\u001b[0;32m     70\u001b[0m         env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores, loss\n\u001b[1;32m---> 75\u001b[0m scores, loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_DDPG\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(scores)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), scores)\n\u001b[0;32m     78\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCumulative reward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[349], line 50\u001b[0m, in \u001b[0;36mrun_DDPG\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# learn when buffer reaches batch size\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ddpg\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m ddpg\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m---> 50\u001b[0m     loss_item \u001b[38;5;241m=\u001b[39m \u001b[43mddpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     loss\u001b[38;5;241m.\u001b[39mappend(loss_item)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# update state\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[336], line 69\u001b[0m, in \u001b[0;36mDDPGAgent.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# sample batch\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     state_batch, action_batch, reward_batch, next_state_batch, dones_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_random_minibatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# calculate target batch\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[329], line 24\u001b[0m, in \u001b[0;36mReplayBuffer.sample_random_minibatch\u001b[1;34m(self, batch_size, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m obs, actions, rewards, next_obs, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Convert lists of tensors into single tensors\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(actions)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(rewards)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: choose device to run on\n",
    "# main loop\n",
    "def run_DDPG():\n",
    "    # initialize parameters\n",
    "    params = {'actor_lr': 0.0001,\n",
    "            'critic_lr': 0.001,\n",
    "            'tau': 0.001,\n",
    "            'gamma': 0.99,\n",
    "            'minibatch_size': 64,\n",
    "            'replay_buffer_size': int(10e6),\n",
    "            'steps': 100_000}\n",
    "\n",
    "    # grab cwd for model saving\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # create environment\n",
    "    env = gym.make('Hopper-v5')\n",
    "\n",
    "    # ddpg object\n",
    "    ddpg = DDPGAgent(env, params, random_seed=10)\n",
    "\n",
    "    # keep track of loss\n",
    "    scores = []\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    loss = []\n",
    "\n",
    "    # loop through desired number of steps\n",
    "    for ep in tqdm.tqdm(range(params['steps']), desc=\"steps\"):\n",
    "        # reset env\n",
    "        state,_ = env.reset()\n",
    "\n",
    "        # initialize terminal state\n",
    "        done = False\n",
    "\n",
    "        # track cumulative reward\n",
    "        cumulative_reward = 0\n",
    "\n",
    "        # while environment isnt terminal\n",
    "        while not done:\n",
    "            # grab action with OU noise\n",
    "            action = ddpg.get_action(state)\n",
    "            # execute action in env\n",
    "            next_state, reward, done, truncate, _ = env.step(action)\n",
    "            # store in buffer\n",
    "            ddpg.replay_buffer.insert(state, action, reward, next_state, done)\n",
    "\n",
    "            # learn when buffer reaches batch size\n",
    "            if len(ddpg.replay_buffer.buffer) > ddpg.batch_size:\n",
    "                loss_item = ddpg.update()\n",
    "                loss.append(loss_item)\n",
    "\n",
    "            # update state\n",
    "            state = next_state\n",
    "\n",
    "            # update cumulative reward\n",
    "            cumulative_reward += reward\n",
    "\n",
    "        # append to running score and to score deque for average reward approximation\n",
    "        scores.append(cumulative_reward)\n",
    "        scores_deque.append(cumulative_reward)\n",
    "\n",
    "        # save scores\n",
    "        if ep % 1000 == 0:\n",
    "            torch.save(ddpg.actor.state_dict(), cwd+'/checkpoint_actor.pth')\n",
    "            torch.save(ddpg.critic.state_dict(), cwd+'/checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(ep, np.mean(scores_deque)))   \n",
    "        \n",
    "        # reset env if done\n",
    "        env.reset()\n",
    "\n",
    "    \n",
    "    return scores, loss\n",
    "\n",
    "scores, loss = run_DDPG()\n",
    "\n",
    "plt.plot(np.arange(1,len(scores)+1), scores)\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\glfw\\__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "# grab cwd for model saving\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# instantiate env\n",
    "env = gym.make('Hopper-v5', render_mode='human')\n",
    "\n",
    "# initialize parameters\n",
    "params = {'actor_lr': 0.0001,\n",
    "        'critic_lr': 0.001,\n",
    "        'tau': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'minibatch_size': 64,\n",
    "        'replay_buffer_size': int(10e6),\n",
    "        'steps': 100_000}\n",
    "\n",
    "ddpg = DDPGAgent(env, params, random_seed=10)\n",
    "\n",
    "ddpg.actor.load_state_dict(torch.load(cwd+'/checkpoint_actor.pth'))\n",
    "ddpg.critic.load_state_dict(torch.load(cwd+'/checkpoint_critic.pth'))\n",
    "\n",
    "state,_ = env.reset()  \n",
    "while True:\n",
    "    action = ddpg.get_action(state)\n",
    "    env.render()\n",
    "    next_state, reward, done, truncate, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
